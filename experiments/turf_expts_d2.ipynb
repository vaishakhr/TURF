{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pylab\n",
    "%matplotlib inline\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import sys\n",
    "sys.path.insert(1,'../src')\n",
    "sys.path.insert(1,'../')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# matplotlib.rcParams['savefig.dpi'] = 1.5 * matplotlib.rcParams['savefig.dpi']\n",
    "from __future__ import print_function\n",
    "from __future__ import division\n",
    "import timeit\n",
    "from ellipsoid import *\n",
    "from newfuns import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We first consider the distributions taken in ADLS namely\n",
    "\n",
    "mixture_params = ((.65, -.45, .15), (.35, .3, .2))\n",
    "gmm = mixture_distribution([normal_distribution(-.45, .15), normal_distribution(.3, .2)], [.65, .35])\n",
    "fig = plot_distribution(gmm, (-1, 1))\n",
    "\n",
    "beta = mixture_distribution([beta_distribution(.8, 4), beta_distribution(2, 2)], [.4, .6])\n",
    "fig = plot_distribution(beta, (0, 1), color='green')\n",
    "\n",
    "gamma = mixture_distribution([gamma_distribution(2.0, 2.0), gamma_distribution(7.5, 1.0)], [.7, .3])\n",
    "fig = plot_distribution(gamma, (0, 20), color='red')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_experiment(distribution, estimator, n_vals, num_trials):\n",
    "    ret_time = np.zeros((len(n_vals), num_trials))\n",
    "    ret_error = np.zeros((len(n_vals), num_trials))\n",
    "    overall_start = timeit.default_timer()\n",
    "    for ii, n in enumerate(n_vals):\n",
    "        print('n = {} ... '.format(n), end='')\n",
    "        start = timeit.default_timer()\n",
    "        total_estimation_time = 0.0\n",
    "        for jj in range(num_trials):\n",
    "            samples = sorted(distribution.draw_samples(n))\n",
    "            result, time = estimator(samples)\n",
    "            total_estimation_time += time\n",
    "            ret_time[ii, jj] = time\n",
    "            ret_error[ii, jj] = piecewise_poly_error_function(result, samples, distribution)\n",
    "        end = timeit.default_timer()\n",
    "        print('total time: {} seconds, total estimation time: {} seconds.'.format(float(end - start), total_estimation_time))\n",
    "    overall_stop = timeit.default_timer()\n",
    "    print('Experiment took {} seconds'.format(float(overall_stop - overall_start)))\n",
    "    return ret_time, ret_error\n",
    "\n",
    "\n",
    "def pp_poly_estimator(distribution, samples, t, d, num_initial_pieces):\n",
    "    n = len(samples)\n",
    "    smin = np.min(samples) - 1e-3\n",
    "    smax = np.max(samples) + 1e-3\n",
    "    U = d * d * (math.sqrt(2) + 1.0)**d\n",
    "    eps = math.sqrt(t * d / float(n))\n",
    "#     print('eps estimate = {}'.format(eps))\n",
    "    akproj_gap_tolerance = eps * eps * 0.5\n",
    "#     print('akproj_gap_tolerance = {}'.format(akproj_gap_tolerance))\n",
    "    #num_pieces = int(math.ceil(t / eps))s\n",
    "#     print('Upper bound = {}  Num initial pieces = {}'.format(U, num_initial_pieces))\n",
    "#     cvxopt.solvers.options['show_progress'] = False\n",
    "\n",
    "    #Vai: what is this tolerance U? seems to be set to d^2(2.4142)^d\n",
    "    start = timeit.default_timer()\n",
    "    res = pp_learning(t, d, num_initial_pieces, (smin, smax), samples, verbose=0, akproj_num_iter=200, akproj_upper_bound=U, akproj_gap_tolerance=akproj_gap_tolerance)\n",
    "    new_res = NewHypothesisPieces(res, t, d, samples)\n",
    "    stop = timeit.default_timer()\n",
    "    err_old = compute_l1_quad(distribution.get_pdf(), get_ppoly_pdf(res), (smin, smax))\n",
    "    err_new = compute_l1_quad(distribution.get_pdf(), get_ppoly_pdf(new_res), (smin, smax))\n",
    "    return [err_old, err_new], float(stop - start)\n",
    "\n",
    "    \n",
    "def piecewise_linear_estimator(samples):\n",
    "    n = len(samples)\n",
    "    smin = np.min(samples) - 1e-3\n",
    "    smax = np.max(samples) + 1e-3\n",
    "    samples_cpp = ellipsoid_cpp.DoubleVector(samples)\n",
    "    \n",
    "    t = 20\n",
    "    eps = math.sqrt(2 * t / float(n))\n",
    "    num_initial_intervals = min(n, int(math.ceil(t / eps)))\n",
    "    num_merged_intervals_holdout = t\n",
    "    max_final_num_intervals = 2 * num_merged_intervals_holdout + 1\n",
    "\n",
    "    h = ellipsoid_cpp.LinearPzieceVector()\n",
    "    opts = ellipsoid_cpp.A1ProjectionOptions()\n",
    "    opts.max_gap = eps / (4.0 * max_final_num_intervals)\n",
    "    opts.max_num_iterations = 20\n",
    "    opts.num_initial_interval_levels = 2\n",
    "    stats = ellipsoid_cpp.A1ProjectionStats()\n",
    "\n",
    "    start = timeit.default_timer()\n",
    "    ellipsoid_cpp.piecewise_linear_approx(samples_cpp, smin, smax, num_initial_intervals, num_merged_intervals_holdout, max_final_num_intervals, opts, h, stats)\n",
    "    stop = timeit.default_timer()\n",
    "    \n",
    "    return convert_piecewise_linear_to_pp_hypothesis(h), float(stop - start)\n",
    "\n",
    "def pp_poly_estimate_explicit(distribution, samples, t, d, num_initial_pieces):\n",
    "    n = len(samples)\n",
    "    smin = np.min(samples) - 1e-3\n",
    "    smax = np.max(samples) + 1e-3\n",
    "    U = d * d * (math.sqrt(2) + 1.0)**d\n",
    "    eps = math.sqrt(t * d / float(n))\n",
    "#     print('eps estimate = {}'.format(eps))\n",
    "    akproj_gap_tolerance = eps * eps * 0.5\n",
    "#     print('akproj_gap_tolerance = {}'.format(akproj_gap_tolerance))\n",
    "    #num_pieces = int(math.ceil(t / eps))s\n",
    "#     print('Upper bound = {}  Num initial pieces = {}'.format(U, num_initial_pieces))\n",
    "#     cvxopt.solvers.options['show_progress'] = False\n",
    "\n",
    "    #Vai: what is this tolerance U? seems to be set to d^2(2.4142)^d\n",
    "    start = timeit.default_timer()\n",
    "#     print('ululu', t, d, num_initial_pieces)\n",
    "    res = pp_learning(t, d, num_initial_pieces, (smin, smax), samples, verbose=0, akproj_num_iter=200, akproj_upper_bound=U, akproj_gap_tolerance=akproj_gap_tolerance)\n",
    "#     print('heeehaa')\n",
    "    new_res = NewHypothesisPieces(res, t, d, samples)\n",
    "    stop = timeit.default_timer()\n",
    "#     err_old = compute_l1_quad(distribution.get_pdf(), get_ppoly_pdf(res), (smin, smax))\n",
    "#     err_new = compute_l1_quad(distribution.get_pdf(), get_ppoly_pdf(new_res), (smin, smax))\n",
    "    return [res, new_res], float(stop - start)\n",
    "\n",
    "\n",
    "# distribution takes distribution input, set estimator = pp_poly_estimator\n",
    "# n_vals is the list of samples to use e.g. [1000, 2000]\n",
    "# num_trials is number of trivals per sample, e.g. 10 or 20\n",
    "# t and d are the usual (function below is not crossvalidated)\n",
    "# num_initial_pieces is the number of initial pieces, take it to be min(n,20*t)\n",
    "\n",
    "def piecewise_poly_error_function(h, samples, distribution):\n",
    "    smin = np.min(samples) - 1e-3\n",
    "    smax = np.max(samples) + 1e-3\n",
    "    return compute_l1_quad(distribution.get_pdf(), get_ppoly_pdf(h), (smin, smax))\n",
    "\n",
    "def new_experiment(distribution, estimator, n_vals, num_trials, t, d, num_initial_pieces):\n",
    "    ret_times = []\n",
    "    ret_error_old = []\n",
    "    ret_error_new = []\n",
    "    for ii, n in enumerate(n_vals):\n",
    "        print('Evaluating the {} distribution with {} samples by averaging over {} runs'.format(distribution[0], n, num_trials))\n",
    "        start = timeit.default_timer()\n",
    "        ret_error_old += [0]\n",
    "        ret_error_new += [0]\n",
    "        estimation_time = 0\n",
    "        for jj in range(num_trials):\n",
    "            samples = sorted(distribution[1].draw_samples(n))\n",
    "            if estimator == pp_poly_estimator:\n",
    "                resses, time = estimator(distribution[1], samples, t, d, num_initial_pieces)\n",
    "            estimation_time += time\n",
    "            ret_error_old[ii] += resses[0]/num_trials\n",
    "            ret_error_new[ii] += resses[1]/num_trials\n",
    "        ret_times += [estimation_time]\n",
    "        print('Error incurred by ADLS is {} and TURF is {}'.format(ret_error_old[ii], ret_error_new[ii]))\n",
    "        print('Time over {} runs is {} seconds \\n'.format(num_trials, estimation_time))\n",
    "    print('Total experiment took {} seconds \\n'.format(sum(ret_times)))\n",
    "    return zip(n_vals, ret_error_old, ret_error_new, ret_times)\n",
    "\n",
    "def newer_cv_experiment(distribution, n_vals, num_trials, d, param):\n",
    "    ret_times = []\n",
    "    ret_error_old = []\n",
    "    ret_error_new = []\n",
    "    for ii, n in enumerate(n_vals):\n",
    "        print('Evaluating the {} distribution with {} samples by averaging over {} runs'.format(distribution[0], n, num_trials))\n",
    "        start = timeit.default_timer()\n",
    "        ret_error_old += [0]\n",
    "        ret_error_new += [0]\n",
    "        estimation_time = 0\n",
    "        # set number of initial pieces to be 100, matching ADLS\n",
    "        num_initial_pieces = 100\n",
    "#         print(\"Number of initial pieces is\", num_initial_pieces)\n",
    "#         print('huhu', num_initial_pieces)\n",
    "        # tceil is set at sqrt since roughly speaking with more than sqrt{n} intervals even collisions won't happen\n",
    "        tceil_exp = int(np.log2(min(np.sqrt(n),100)))\n",
    "        for jj in range(num_trials):\n",
    "#             print('Running trial {} with {} samples, number of initial pieces {}'.format(jj, n, num_initial_pieces))\n",
    "            samples = sorted(distribution[1].draw_samples(n))\n",
    "            resses, time = toss_validator(distribution[1], samples, tceil_exp, d, param, num_initial_pieces)\n",
    "            estimation_time += time\n",
    "            ret_error_old[ii] += resses[0]/num_trials\n",
    "            ret_error_new[ii] += resses[1]/num_trials\n",
    "        ret_times += [estimation_time]\n",
    "        print('Error incurred by ADLS is {} and TURF is {}'.format(ret_error_old[ii], ret_error_new[ii]))\n",
    "        print('Time over {} runs is {} seconds \\n'.format(num_trials, estimation_time))\n",
    "    print('Total experiment took {} seconds \\n'.format(sum(ret_times)))\n",
    "    return zip(n_vals, ret_error_old, ret_error_new, ret_times)\n",
    "\n",
    "\n",
    "def toss_validator(distribution, samples, tceil_exp, d, param, num_initial_pieces):\n",
    "    smin = np.min(samples) - 1e-3\n",
    "    smax = np.max(samples) + 1e-3\n",
    "    tot_time = 0\n",
    "    time_est = 0\n",
    "    time_cv = 0\n",
    "    best_err_old = 2\n",
    "    best_err_new = 2\n",
    "    const = param\n",
    "    estim_set = []\n",
    "    for t in range(tceil_exp):\n",
    "#         num_initial_pieces = 50\n",
    "#         print('lalallalala', 2**(t+1), num_initial_pieces)\n",
    "# number of pieces varies between 2 and 2^tceil_exp\n",
    "        estes, time = pp_poly_estimate_explicit(distribution, samples, 2**(t+1), d, num_initial_pieces)\n",
    "        estim_set = estim_set + [estes]\n",
    "        time_est += time\n",
    "#     print('Time taken to compute all estimators is ', time_est)\n",
    "\n",
    "    start = timeit.default_timer()\n",
    "    \n",
    "    flag_old = 0\n",
    "    for t in range(tceil_exp):\n",
    "        cur_estim_old = estim_set[t][0]\n",
    "        if flag_old == 1:\n",
    "            break\n",
    "#         print('Current ADLS candidate with d = {0} and t = {1}'.format(d, 2**(t+1)))\n",
    "        for tt in range(t, tceil_exp):\n",
    "#             print('bobo')\n",
    "            check_estim_old = estim_set[tt][0]\n",
    "            delta_old = compute_l1_quad(get_ppoly_pdf(cur_estim_old), get_ppoly_pdf(check_estim_old), (smin, smax))\n",
    "# CV parameter is prop to sqrt of the number of pieces used - a meaningful heauristic\n",
    "            thresh_old = param*np.sqrt(len(check_estim_old))*np.sqrt((2**d+1)/len(samples))\n",
    "            if delta_old > thresh_old:\n",
    "                break\n",
    "# flag takes value 1 if checks have passed for all tt>t\n",
    "            if tt == tceil_exp - 1:\n",
    "                flag_old = 1\n",
    "#                 print('Value of t that passed all tests is', t)\n",
    "                \n",
    "\n",
    "    flag_new = 0          \n",
    "    for t in range(tceil_exp):\n",
    "        cur_estim_new = estim_set[t][1]\n",
    "        if flag_new == 1:\n",
    "            break\n",
    "#         print('Current TURF candidate with d = {0} and t = {1}'.format(d, 2**(t+1)))\n",
    "        for tt in range(t, tceil_exp):\n",
    "#             print('bobobo')\n",
    "            check_estim_new = estim_set[tt][1]\n",
    "            delta_new = compute_l1_quad(get_ppoly_pdf(cur_estim_new), get_ppoly_pdf(check_estim_new), (smin, smax))\n",
    "# CV parameter is prop to sqrt of the number of pieces used - a meaningful heauristic\n",
    "            thresh_new = param*np.sqrt(len(check_estim_new))*np.sqrt((2**d+1)/len(samples))\n",
    "            if delta_new > thresh_new:\n",
    "                break\n",
    "# flag takes value 1 if checks have passed for all tt>t\n",
    "            if tt == tceil_exp - 1:\n",
    "                flag_new = 1\n",
    "#                 print('Value of t that passed all tests is', t)\n",
    "    stop = timeit.default_timer()\n",
    "    time_cv = stop - start\n",
    "#     print('Time taken to cross validate is', time_cv)\n",
    "    tot_time = time_est + time_cv\n",
    "    best_err_old = compute_l1_quad(distribution.get_pdf(), get_ppoly_pdf(cur_estim_old), (smin, smax))\n",
    "    best_err_new = compute_l1_quad(distribution.get_pdf(), get_ppoly_pdf(cur_estim_new), (smin, smax))  \n",
    "    return [best_err_old, best_err_new], tot_time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now consider noisy distributions, i.e. same as above but perturbed by Gaussian noise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# generating noisy distributions\n",
    "# k refers to the number of Gaussian noise components\n",
    "k = 100\n",
    "\n",
    "means = list(np.random.uniform(-1,1,k))\n",
    "mix = []\n",
    "dist_mix = []\n",
    "for i in range(k):\n",
    "    mix += [0.25/k]\n",
    "    dist_mix += [normal_distribution(means[i], 0.1/np.sqrt(k))]\n",
    "\n",
    "# noise is added to the ADLS distributions, defined again for completeness\n",
    "\n",
    "mixture_params = ((.65, -.45, .15), (.35, .3, .2))\n",
    "gmm = mixture_distribution([normal_distribution(-.45, .15), normal_distribution(.3, .2)], [.65, .35])\n",
    "\n",
    "full_mix = [0.75] + mix\n",
    "full_dist_mix = [gmm] + dist_mix\n",
    "gmmk = mixture_distribution(full_dist_mix, full_mix)\n",
    "fig = plot_distribution(gmmk, (-1,1))\n",
    "\n",
    "\n",
    "means = list(np.random.uniform(0,1,k))\n",
    "mix = []\n",
    "dist_mix = []\n",
    "for i in range(k):\n",
    "    mix += [0.25/k]\n",
    "    dist_mix += [normal_distribution(means[i],.05/np.sqrt(k))]\n",
    "\n",
    "beta = mixture_distribution([beta_distribution(.8, 4), beta_distribution(2, 2)], [.4, .6])\n",
    "\n",
    "full_mix = [0.75] + mix\n",
    "full_dist_mix = [beta] + dist_mix\n",
    "betak = mixture_distribution(full_dist_mix, full_mix)\n",
    "fig = plot_distribution(betak, (0,1), color='green')\n",
    "\n",
    "\n",
    "means = list(np.random.uniform(0,20,k))\n",
    "mix = []\n",
    "dist_mix = []\n",
    "for i in range(k):\n",
    "    mix += [0.25/k]\n",
    "    dist_mix += [normal_distribution(means[i], 1/np.sqrt(k))]\n",
    "    \n",
    "gamma = mixture_distribution([gamma_distribution(2.0, 2.0), gamma_distribution(7.5, 1.0)], [.7, .3])\n",
    "\n",
    "full_mix = [0.75] + mix\n",
    "full_dist_mix = [gamma] + dist_mix\n",
    "gammak = mixture_distribution(full_dist_mix, full_mix)\n",
    "fig = plot_distribution(gammak, (0,20), color='red')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# cv over t\n",
    "# n_vals = [1000, 2000]\n",
    "n_vals = [1000, 2000, 5000, 10000, 20000, 40000, 80000]\n",
    "num_trials = 100\n",
    "distributions = [('beta', beta), ('gamma', gamma), ('gmm', gmm)]\n",
    "\n",
    "# greatest t, tceil\n",
    "# fixed d\n",
    "d = 2\n",
    "# param is the multiple used in CV. A large value leads to few pieces\n",
    "param = 5\n",
    "for dist in distributions:\n",
    "    expt_out = newer_cv_experiment(dist, n_vals, num_trials, d, param)\n",
    "    res_adls = []\n",
    "    res_turf = []\n",
    "    for vec in expt_out:\n",
    "        res_adls += [(vec[0], vec[1])]\n",
    "        res_turf += [(vec[0], vec[2])]\n",
    "    print('ADLS with d={0} on {1}'.format(d, dist[0]))\n",
    "    print(res_adls)\n",
    "    print('TURF with d={0} on {1}'.format(d, dist[0]))\n",
    "    print(res_turf)\n",
    "    print('\\n')\n",
    "    print('-----------------------------------------------------------------')\n",
    "    print('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# cv over t\n",
    "# n_vals = [1000, 2000]\n",
    "n_vals = [1000, 2000, 5000, 10000, 20000, 40000, 80000]\n",
    "num_trials = 100\n",
    "distributions = [('betak', betak), ('gammak', gammak), ('gmm', gmmk)]\n",
    "\n",
    "# greatest t, tceil\n",
    "# fixed d\n",
    "d = 2\n",
    "# param is the multiple used in CV. A large value leads to few pieces\n",
    "param = 5\n",
    "for dist in distributions:\n",
    "    expt_out = newer_cv_experiment(dist, n_vals, num_trials, d, param)\n",
    "    res_adls = []\n",
    "    res_turf = []\n",
    "    for vec in expt_out:\n",
    "        res_adls += [(vec[0], vec[1])]\n",
    "        res_turf += [(vec[0], vec[2])]\n",
    "    print('ADLS with d={0} on {1}'.format(d, dist[0]))\n",
    "    print(res_adls)\n",
    "    print('TURF with d={0} on {1}'.format(d, dist[0]))\n",
    "    print(res_turf)\n",
    "    print('\\n')\n",
    "    print('-----------------------------------------------------------------')\n",
    "    print('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
